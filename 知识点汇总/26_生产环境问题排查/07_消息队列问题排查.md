# æ¶ˆæ¯é˜Ÿåˆ—é—®é¢˜æ’æŸ¥

> å®æˆ˜æŒ‡å—ï¼šå¦‚ä½•æ’æŸ¥æ¶ˆæ¯ç§¯å‹ã€æ¶ˆè´¹å»¶è¿Ÿã€é‡å¤æ¶ˆè´¹ç­‰æ¶ˆæ¯é˜Ÿåˆ—é—®é¢˜

## ğŸ“‹ ç›®å½•
- [å¸¸è§é—®é¢˜ç±»å‹](#å¸¸è§é—®é¢˜ç±»å‹)
- [æ¶ˆæ¯ç§¯å‹](#æ¶ˆæ¯ç§¯å‹)
- [æ¶ˆè´¹å»¶è¿Ÿ](#æ¶ˆè´¹å»¶è¿Ÿ)
- [é‡å¤æ¶ˆè´¹](#é‡å¤æ¶ˆè´¹)
- [æ¶ˆæ¯ä¸¢å¤±](#æ¶ˆæ¯ä¸¢å¤±)

---

## å¸¸è§é—®é¢˜ç±»å‹

### 1. æ¶ˆæ¯ç§¯å‹

**ç—‡çŠ¶**ï¼š
```
âœ… é˜Ÿåˆ—é•¿åº¦æŒç»­å¢é•¿
âœ… æ¶ˆè´¹é€Ÿåº¦è·Ÿä¸ä¸Šç”Ÿäº§é€Ÿåº¦
âœ… æ¶ˆæ¯å»¶è¿Ÿå¢åŠ 
```

### 2. æ¶ˆè´¹å»¶è¿Ÿ

**ç—‡çŠ¶**ï¼š
```
âœ… æ¶ˆæ¯å¤„ç†å»¶è¿Ÿ
âœ… æ¶ˆè´¹é€Ÿåº¦æ…¢
âœ… æ¶ˆè´¹è€…æ€§èƒ½ä¸‹é™
```

### 3. é‡å¤æ¶ˆè´¹

**ç—‡çŠ¶**ï¼š
```
âœ… åŒä¸€æ¡æ¶ˆæ¯è¢«æ¶ˆè´¹å¤šæ¬¡
âœ… ä¸šåŠ¡æ•°æ®é‡å¤
âœ… å¹‚ç­‰æ€§é—®é¢˜
```

### 4. æ¶ˆæ¯ä¸¢å¤±

**ç—‡çŠ¶**ï¼š
```
âœ… æ¶ˆæ¯æœªæ¶ˆè´¹å°±æ¶ˆå¤±
âœ… ä¸šåŠ¡æ•°æ®ä¸¢å¤±
âœ… æ¶ˆæ¯æœªæŒä¹…åŒ–
```

---

## æ¶ˆæ¯ç§¯å‹

### æ’æŸ¥æ–¹æ³•

**1. æŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦**

```bash
# KafkaæŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦
kafka-run-class kafka.tools.ConsumerGroupCommand \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --describe

# RocketMQæŸ¥çœ‹é˜Ÿåˆ—é•¿åº¦
mqadmin consumerProgress \
  -n localhost:9876 \
  -g my-group
```

**2. æŸ¥çœ‹æ¶ˆè´¹é€Ÿåº¦**

```bash
# KafkaæŸ¥çœ‹æ¶ˆè´¹é€Ÿåº¦
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --describe

# å…³æ³¨ï¼š
# LAG: æ¶ˆæ¯ç§¯å‹æ•°é‡
# Current Offset: å½“å‰æ¶ˆè´¹ä½ç½®
# Log End Offset: æ¶ˆæ¯é˜Ÿåˆ—æœ«å°¾ä½ç½®
```

### å¸¸è§åŸå› 

**1. æ¶ˆè´¹é€Ÿåº¦æ…¢**

```
- æ¶ˆè´¹è€…æ€§èƒ½ä¸è¶³
- ä¸šåŠ¡é€»è¾‘å¤æ‚
- æ•°æ®åº“æ“ä½œæ…¢
- ç½‘ç»œå»¶è¿Ÿ
```

**2. æ¶ˆè´¹è€…æ•…éšœ**

```
- æ¶ˆè´¹è€…å®•æœº
- æ¶ˆè´¹è€…é‡å¯
- æ¶ˆè´¹è€…å¼‚å¸¸é€€å‡º
```

**3. ç”Ÿäº§é€Ÿåº¦è¿‡å¿«**

```
- çªå‘æµé‡
- æ‰¹é‡ç”Ÿäº§
- ç”Ÿäº§é€Ÿåº¦è¶…è¿‡æ¶ˆè´¹é€Ÿåº¦
```

### è§£å†³æ–¹æ¡ˆ

**1. å¢åŠ æ¶ˆè´¹è€…**

```java
// å¢åŠ æ¶ˆè´¹è€…å®ä¾‹
// Kafka: å¢åŠ partitionæ•°é‡æˆ–æ¶ˆè´¹è€…æ•°é‡
// RocketMQ: å¢åŠ æ¶ˆè´¹è€…æ•°é‡
```

**2. ä¼˜åŒ–æ¶ˆè´¹é€»è¾‘**

```java
// âŒ ä¸²è¡Œå¤„ç†
for (Message message : messages) {
    processMessage(message);  // æ…¢
}

// âœ… å¹¶è¡Œå¤„ç†
messages.parallelStream().forEach(this::processMessage);
```

**3. æ‰¹é‡æ¶ˆè´¹**

```java
// æ‰¹é‡æ¶ˆè´¹ï¼Œæé«˜ååé‡
@KafkaListener(topics = "topic", concurrency = "10")
public void consume(List<ConsumerRecord<String, String>> records) {
    // æ‰¹é‡å¤„ç†
    processBatch(records);
}
```

**4. é™æµä¿æŠ¤**

```java
// é™æµä¿æŠ¤ï¼Œé¿å…æ¶ˆè´¹è¿‡å¿«å¯¼è‡´ä¸‹æ¸¸å‹åŠ›
RateLimiter limiter = RateLimiter.create(100);  // 100 QPS
for (Message message : messages) {
    limiter.acquire();
    processMessage(message);
}
```

---

## æ¶ˆè´¹å»¶è¿Ÿ

### æ’æŸ¥æ–¹æ³•

**1. æŸ¥çœ‹æ¶ˆè´¹å»¶è¿Ÿ**

```bash
# KafkaæŸ¥çœ‹æ¶ˆè´¹å»¶è¿Ÿ
kafka-consumer-groups.sh \
  --bootstrap-server localhost:9092 \
  --group my-group \
  --describe

# å…³æ³¨LAGï¼ˆç§¯å‹æ•°é‡ï¼‰
```

**2. ç›‘æ§æ¶ˆè´¹æ€§èƒ½**

```java
// ç›‘æ§æ¶ˆè´¹è€—æ—¶
long startTime = System.currentTimeMillis();
processMessage(message);
long duration = System.currentTimeMillis() - startTime;
log.info("æ¶ˆè´¹è€—æ—¶: {}ms", duration);
```

### å¸¸è§åŸå› 

**1. æ¶ˆè´¹é€»è¾‘å¤æ‚**

```java
// âŒ æ¶ˆè´¹é€»è¾‘å¤æ‚ï¼Œè€—æ—¶è¾ƒé•¿
public void consume(Message message) {
    // å¤æ‚ä¸šåŠ¡é€»è¾‘
    // æ•°æ®åº“æ“ä½œ
    // å¤–éƒ¨æœåŠ¡è°ƒç”¨
}

// âœ… ä¼˜åŒ–æ¶ˆè´¹é€»è¾‘
public void consume(Message message) {
    // å¿«é€Ÿå¤„ç†
    // å¼‚æ­¥å¤„ç†å¤æ‚é€»è¾‘
    asyncProcess(message);
}
```

**2. æ•°æ®åº“æ“ä½œæ…¢**

```java
// âŒ åŒæ­¥æ•°æ®åº“æ“ä½œ
public void consume(Message message) {
    db.insert(message);  // æ…¢
}

// âœ… æ‰¹é‡æ’å…¥
public void consume(List<Message> messages) {
    db.batchInsert(messages);  // å¿«
}
```

**3. æ¶ˆè´¹è€…èµ„æºä¸è¶³**

```bash
# æ£€æŸ¥æ¶ˆè´¹è€…èµ„æº
top
iostat -x 1
```

### è§£å†³æ–¹æ¡ˆ

**1. ä¼˜åŒ–æ¶ˆè´¹é€»è¾‘**

```java
// å‡å°‘åŒæ­¥æ“ä½œ
// ä½¿ç”¨å¼‚æ­¥å¤„ç†
// æ‰¹é‡å¤„ç†
```

**2. å¢åŠ æ¶ˆè´¹è€…èµ„æº**

```
- å¢åŠ CPUæ ¸å¿ƒæ•°
- å¢åŠ å†…å­˜
- ä¼˜åŒ–ç½‘ç»œ
```

**3. ä½¿ç”¨å¼‚æ­¥å¤„ç†**

```java
// å¼‚æ­¥å¤„ç†å¤æ‚é€»è¾‘
@KafkaListener(topics = "topic")
public void consume(Message message) {
    // å¿«é€Ÿè¿”å›
    asyncService.process(message);
}
```

---

## é‡å¤æ¶ˆè´¹

### æ’æŸ¥æ–¹æ³•

**1. æŸ¥çœ‹æ¶ˆæ¯ID**

```java
// è®°å½•æ¶ˆæ¯IDï¼Œæ£€æŸ¥æ˜¯å¦é‡å¤
String messageId = message.getMessageId();
if (processedIds.contains(messageId)) {
    log.warn("é‡å¤æ¶ˆè´¹: {}", messageId);
    return;  // è·³è¿‡
}
processedIds.add(messageId);
```

**2. æŸ¥çœ‹æ¶ˆè´¹æ—¥å¿—**

```bash
# æŸ¥çœ‹æ¶ˆè´¹æ—¥å¿—
grep "é‡å¤æ¶ˆè´¹" application.log
grep "messageId" application.log | sort | uniq -d
```

### å¸¸è§åŸå› 

**1. æ¶ˆè´¹å¤±è´¥é‡è¯•**

```
- æ¶ˆè´¹å¤±è´¥åé‡è¯•
- æ¶ˆæ¯è¢«é‡å¤æŠ•é€’
- æ¶ˆè´¹è€…é‡å¯åé‡å¤æ¶ˆè´¹
```

**2. æ¶ˆæ¯é˜Ÿåˆ—æœºåˆ¶**

```
- Kafka: æ¶ˆè´¹è€…åç§»é‡æœªæäº¤
- RocketMQ: æ¶ˆæ¯é‡è¯•æœºåˆ¶
```

### è§£å†³æ–¹æ¡ˆ

**1. å¹‚ç­‰æ€§è®¾è®¡**

```java
// ä½¿ç”¨å”¯ä¸€æ ‡è¯†ä¿è¯å¹‚ç­‰æ€§
String messageId = message.getMessageId();
String key = "processed:" + messageId;

// ä½¿ç”¨Redisæ£€æŸ¥æ˜¯å¦å·²å¤„ç†
if (redis.exists(key)) {
    return;  // å·²å¤„ç†ï¼Œè·³è¿‡
}

// å¤„ç†æ¶ˆæ¯
processMessage(message);

// æ ‡è®°å·²å¤„ç†
redis.setex(key, 3600, "1");  // 1å°æ—¶è¿‡æœŸ
```

**2. æ•°æ®åº“å”¯ä¸€çº¦æŸ**

```sql
-- ä½¿ç”¨å”¯ä¸€çº¦æŸé˜²æ­¢é‡å¤
CREATE TABLE message_processed (
    message_id VARCHAR(64) PRIMARY KEY,
    processed_time TIMESTAMP
);
```

**3. åˆ†å¸ƒå¼é”**

```java
// ä½¿ç”¨åˆ†å¸ƒå¼é”ä¿è¯å¹‚ç­‰æ€§
String lockKey = "lock:" + messageId;
if (redis.setnx(lockKey, "1", 60)) {  // è·å–é”
    try {
        processMessage(message);
    } finally {
        redis.del(lockKey);  // é‡Šæ”¾é”
    }
} else {
    log.warn("æ¶ˆæ¯æ­£åœ¨å¤„ç†ä¸­: {}", messageId);
}
```

---

## æ¶ˆæ¯ä¸¢å¤±

### æ’æŸ¥æ–¹æ³•

**1. æŸ¥çœ‹æ¶ˆæ¯ç¡®è®¤**

```java
// Kafka: æ‰‹åŠ¨æäº¤åç§»é‡
@KafkaListener(topics = "topic")
public void consume(ConsumerRecord<String, String> record) {
    try {
        processMessage(record);
        // å¤„ç†æˆåŠŸåæ‰æäº¤åç§»é‡
        // æ³¨æ„ï¼šå¦‚æœè¿™é‡ŒæŠ›å‡ºå¼‚å¸¸ï¼Œæ¶ˆæ¯ä¼šä¸¢å¤±
    } catch (Exception e) {
        log.error("å¤„ç†å¤±è´¥", e);
        // å¤„ç†å¤±è´¥ï¼Œä¸æäº¤åç§»é‡ï¼Œæ¶ˆæ¯ä¼šé‡æ–°æ¶ˆè´¹
    }
}
```

**2. æŸ¥çœ‹æ¶ˆæ¯æŒä¹…åŒ–**

```bash
# KafkaæŸ¥çœ‹æ¶ˆæ¯æŒä¹…åŒ–
kafka-run-class kafka.tools.DumpLogSegments \
  --files /tmp/kafka-logs/topic-0/00000000000000000000.log \
  --print-data-log
```

### å¸¸è§åŸå› 

**1. æ¶ˆæ¯æœªæŒä¹…åŒ–**

```
- æ¶ˆæ¯é˜Ÿåˆ—å®•æœº
- æ¶ˆæ¯æœªè½ç›˜
- æ¶ˆæ¯åœ¨å†…å­˜ä¸­ä¸¢å¤±
```

**2. æ¶ˆè´¹å¤±è´¥æœªå¤„ç†**

```
- æ¶ˆè´¹å¤±è´¥åæœªé‡è¯•
- å¼‚å¸¸è¢«åæ‰
- æ¶ˆæ¯è¢«ä¸¢å¼ƒ
```

**3. åç§»é‡æäº¤é”™è¯¯**

```java
// âŒ å¤„ç†å¤±è´¥ä½†æäº¤äº†åç§»é‡
try {
    processMessage(message);
} catch (Exception e) {
    // å¼‚å¸¸è¢«åæ‰
}
// åç§»é‡è¢«æäº¤ï¼Œæ¶ˆæ¯ä¸¢å¤±

// âœ… å¤„ç†å¤±è´¥ä¸æäº¤åç§»é‡
try {
    processMessage(message);
} catch (Exception e) {
    log.error("å¤„ç†å¤±è´¥", e);
    throw e;  // æŠ›å‡ºå¼‚å¸¸ï¼Œä¸æäº¤åç§»é‡
}
```

### è§£å†³æ–¹æ¡ˆ

**1. æ¶ˆæ¯æŒä¹…åŒ–**

```java
// Kafka: è®¾ç½®acks=allï¼Œä¿è¯æ¶ˆæ¯æŒä¹…åŒ–
properties.put("acks", "all");
properties.put("retries", 3);
properties.put("max.in.flight.requests.per.connection", 1);
```

**2. æ‰‹åŠ¨æäº¤åç§»é‡**

```java
// æ‰‹åŠ¨æäº¤åç§»é‡ï¼Œä¿è¯æ¶ˆæ¯å¤„ç†æˆåŠŸ
@KafkaListener(topics = "topic", 
               containerFactory = "kafkaListenerContainerFactory")
public void consume(ConsumerRecord<String, String> record,
                    Acknowledgment ack) {
    try {
        processMessage(record);
        ack.acknowledge();  // å¤„ç†æˆåŠŸåæäº¤
    } catch (Exception e) {
        log.error("å¤„ç†å¤±è´¥", e);
        // ä¸æäº¤åç§»é‡ï¼Œæ¶ˆæ¯ä¼šé‡æ–°æ¶ˆè´¹
    }
}
```

**3. æ­»ä¿¡é˜Ÿåˆ—**

```java
// å¤„ç†å¤±è´¥çš„æ¶ˆæ¯å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
try {
    processMessage(message);
} catch (Exception e) {
    log.error("å¤„ç†å¤±è´¥", e);
    // å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
    deadLetterQueue.send(message);
}
```

---

## æ¶ˆæ¯é˜Ÿåˆ—ä¼˜åŒ–å»ºè®®

### 1. æ¶ˆè´¹ä¼˜åŒ–

```
âœ… æ‰¹é‡æ¶ˆè´¹
âœ… å¹¶è¡Œå¤„ç†
âœ… å¼‚æ­¥å¤„ç†
âœ… ä¼˜åŒ–æ¶ˆè´¹é€»è¾‘
```

### 2. ç›‘æ§å‘Šè­¦

```
âœ… é˜Ÿåˆ—é•¿åº¦ç›‘æ§
âœ… æ¶ˆè´¹å»¶è¿Ÿç›‘æ§
âœ… æ¶ˆè´¹é€Ÿåº¦ç›‘æ§
âœ… é”™è¯¯ç‡ç›‘æ§
```

### 3. å®¹é”™è®¾è®¡

```
âœ… å¹‚ç­‰æ€§è®¾è®¡
âœ… é‡è¯•æœºåˆ¶
âœ… æ­»ä¿¡é˜Ÿåˆ—
âœ… æ¶ˆæ¯æŒä¹…åŒ–
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [æ€§èƒ½é—®é¢˜æ’æŸ¥](./01_æ€§èƒ½é—®é¢˜æ’æŸ¥.md)
- [Kafkaæ ¸å¿ƒæœºåˆ¶è¯¦è§£](../../05_æ¶ˆæ¯é˜Ÿåˆ—/Kafkaæ ¸å¿ƒæœºåˆ¶è¯¦è§£.md)
- [RocketMQæ ¸å¿ƒæœºåˆ¶è¯¦è§£](../../05_æ¶ˆæ¯é˜Ÿåˆ—/RocketMQæ ¸å¿ƒæœºåˆ¶è¯¦è§£.md)

---

**æœ€åæ›´æ–°**: 2025-10-29  
**æ–‡æ¡£çŠ¶æ€**: âœ… æ¡†æ¶å·²æ­å»ºï¼Œå†…å®¹æŒç»­å®Œå–„ä¸­


### æ¶ˆæ¯ç§¯å‹é—®é¢˜å¤„ç†

**1. æ¶ˆæ¯ç§¯å‹ç›‘æ§å‘Šè­¦**

```java
// Prometheusç›‘æ§æŒ‡æ ‡å®šä¹‰
@Timed(value = "mq.consumer.message.backlog", description = "æ¶ˆæ¯ç§¯å‹æ•°é‡")
public void recordMessageBacklog(String topic, long backlog) {
    // æŒ‡æ ‡è®°å½•å®ç°
}

// å‘Šè­¦è§„åˆ™é…ç½® (Prometheus Rule)
groups:
- name: mq_alerts
  rules:
  - alert: MessageBacklogTooHigh
    expr: mq_consumer_message_backlog > 10000
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "æ¶ˆæ¯ç§¯å‹è¿‡å¤š"
      description: "Topic {{ $labels.topic }} ç§¯å‹æ¶ˆæ¯æ•° {{ $value }}"
```

**2. ä¸´æ—¶æ‰©å®¹æ¶ˆè´¹èƒ½åŠ›**

```bash
# Kafkaä¸´æ—¶å¢åŠ æ¶ˆè´¹ç»„åˆ†åŒºæ•°
kafka-topics.sh --bootstrap-server localhost:9092 --alter --topic order_topic --partitions 10

# RabbitMQåŠ¨æ€è°ƒæ•´æ¶ˆè´¹è€…æ•°é‡
rabbitmqctl set_policy -p / my_policy "^order_topic$" '{"ha-mode":"all","queue-max-length":100000}' --apply-to queues

# å¯åŠ¨ä¸´æ—¶æ¶ˆè´¹ç¨‹åºï¼ˆJavaç¤ºä¾‹ï¼‰
java -jar consumer-bootstrap.jar --spring.profiles.active=emergency --mq.topic=order_topic --mq.consumer.count=20
```

**3. æ¶ˆæ¯ç§¯å‹å¤„ç†æµç¨‹**

```java
@Service
public class EmergencyMessageConsumer {
    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;
    
    // å¿«é€Ÿè·³è¿‡æ— æ•ˆæ¶ˆæ¯
    @KafkaListener(topics = "order_topic", groupId = "emergency_consumer")
    public void consume(ConsumerRecord<String, String> record, Acknowledgment ack) {
        try {
            // 1. æ£€æŸ¥æ¶ˆæ¯æ—¶é—´æˆ³ï¼Œè·³è¿‡è¿‡æœŸæ¶ˆæ¯
            long messageTimestamp = record.timestamp();
            if (System.currentTimeMillis() - messageTimestamp > 3600_000) { // 1å°æ—¶è¿‡æœŸ
                log.warn("è·³è¿‡è¿‡æœŸæ¶ˆæ¯: {}", record.offset());
                ack.acknowledge();
                return;
            }
            
            // 2. å¿«é€Ÿå¤„ç†æˆ–è½¬å‘åˆ°ä¸´æ—¶é˜Ÿåˆ—
            processMessage(record.value());
            ack.acknowledge();
        } catch (Exception e) {
            // 3. å¼‚å¸¸æ¶ˆæ¯è½¬å‘åˆ°æ­»ä¿¡é˜Ÿåˆ—
            kafkaTemplate.send("order_topic_dlq", record.value());
            ack.acknowledge();
        }
    }
}
```

---

### æ­»ä¿¡é˜Ÿåˆ—é…ç½®ä¸å¤„ç†

**1. Kafkaæ­»ä¿¡é˜Ÿåˆ—å®ç°**

```java
@Configuration
public class KafkaDLQConfig {
    @Bean
    public NewTopic orderTopicDLQ() {
        return TopicBuilder.name("order_topic_dlq")
                .partitions(3)
                .replicas(2)
                .config(TopicConfig.RETENTION_MS_CONFIG, "604800000") // 7å¤©ä¿ç•™
                .build();
    }
}

// æ­»ä¿¡æ¶ˆæ¯å¤„ç†
@KafkaListener(topics = "order_topic_dlq", groupId = "dlq_consumer")
public void handleDLQMessage(ConsumerRecord<String, String> record) {
    log.error("å¤„ç†æ­»ä¿¡æ¶ˆæ¯: offset={}, value={}", record.offset(), record.value());
    // 1. æ¶ˆæ¯å­˜å‚¨åˆ°æ•°æ®åº“ä»¥ä¾¿äººå·¥åˆ†æ
    dlqMessageRepository.save(new DLQMessage(record));
    // 2. å°è¯•é‡æ–°å¤„ç†
    retryProcess(record.value());
}
```

**2. RabbitMQæ­»ä¿¡é˜Ÿåˆ—é…ç½®**

```java
@Configuration
public class RabbitMQDLQConfig {
    // ä¸šåŠ¡é˜Ÿåˆ—
    @Bean
    public Queue orderQueue() {
        Map<String, Object> args = new HashMap<>();
        // è®¾ç½®æ­»ä¿¡äº¤æ¢æœº
        args.put("x-dead-letter-exchange", "order_dlq_exchange");
        // è®¾ç½®æ­»ä¿¡è·¯ç”±é”®
        args.put("x-dead-letter-routing-key", "order.dlq");
        // è®¾ç½®é˜Ÿåˆ—æœ€å¤§é•¿åº¦
        args.put("x-max-length", 100000);
        // è®¾ç½®æ¶ˆæ¯è¿‡æœŸæ—¶é—´(æ¯«ç§’)
        args.put("x-message-ttl", 60000);
        return QueueBuilder.durable("order_queue").withArguments(args).build();
    }
    
    // æ­»ä¿¡äº¤æ¢æœº
    @Bean
    public DirectExchange orderDLQExchange() {
        return ExchangeBuilder.directExchange("order_dlq_exchange").durable(true).build();
    }
    
    // æ­»ä¿¡é˜Ÿåˆ—
    @Bean
    public Queue orderDLQQueue() {
        return QueueBuilder.durable("order_dlq_queue").build();
    }
    
    // æ­»ä¿¡é˜Ÿåˆ—ç»‘å®š
    @Bean
    public Binding orderDLQBinding() {
        return BindingBuilder.bind(orderDLQQueue())
                .to(orderDLQExchange())
                .with("order.dlq");
    }
}
```

---

### æ¶ˆæ¯å¹‚ç­‰æ€§è§£å†³æ–¹æ¡ˆ

**1. åŸºäºæ•°æ®åº“å”¯ä¸€ç´¢å¼•å®ç°**

```java
@Service
public class OrderService {
    @Autowired
    private OrderMapper orderMapper;
    
    @Transactional
    public void processOrder(String messageId, OrderDTO orderDTO) {
        // 1. æ’å…¥æ¶ˆæ¯æ—¥å¿—ï¼Œåˆ©ç”¨å”¯ä¸€ç´¢å¼•é˜²æ­¢é‡å¤å¤„ç†
        MessageLog log = new MessageLog();
        log.setMessageId(messageId);
        log.setStatus(1); // å¤„ç†ä¸­
        
        try {
            messageLogMapper.insert(log);
        } catch (DuplicateKeyException e) {
            // å”¯ä¸€ç´¢å¼•å†²çªï¼Œè¯´æ˜å·²å¤„ç†è¿‡
            log.warn("æ¶ˆæ¯å·²å¤„ç†: {}", messageId);
            return;
        }
        
        try {
            // 2. ä¸šåŠ¡å¤„ç†
            orderMapper.insert(orderDTO);
            // 3. æ›´æ–°æ¶ˆæ¯çŠ¶æ€ä¸ºæˆåŠŸ
            log.setStatus(2);
            messageLogMapper.updateById(log);
        } catch (Exception e) {
            // 4. æ›´æ–°æ¶ˆæ¯çŠ¶æ€ä¸ºå¤±è´¥
            log.setStatus(3);
            log.setErrorMsg(e.getMessage());
            messageLogMapper.updateById(log);
            throw e;
        }
    }
}
```

**2. åŸºäºRediså®ç°åˆ†å¸ƒå¼é”**

```java
@Component
public class IdempotentProcessor {
    @Autowired
    private StringRedisTemplate redisTemplate;
    
    // å¤„ç†å¹‚ç­‰æ€§ä»»åŠ¡
    public <T, R> R process(String key, T data, Function<T, R> processor) {
        // 1. è·å–åˆ†å¸ƒå¼é”
        Boolean locked = redisTemplate.opsForValue().setIfAbsent(
            "idempotent:" + key,
            "processing",
            30, TimeUnit.SECONDS
        );
        
        if (Boolean.TRUE.equals(locked)) {
            try {
                // 2. æ£€æŸ¥æ˜¯å¦å·²å¤„ç†
                String result = redisTemplate.opsForValue().get("result:" + key);
                if (result != null) {
                    return JSON.parseObject(result, (TypeReference<R>) new TypeReference<R>() {});
                }
                
                // 3. å¤„ç†ä¸šåŠ¡
                R processResult = processor.apply(data);
                
                // 4. å­˜å‚¨å¤„ç†ç»“æœ
                redisTemplate.opsForValue().set(
                    "result:" + key,
                    JSON.toJSONString(processResult),
                    24, TimeUnit.HOURS
                );
                
                return processResult;
            } finally {
                // 5. é‡Šæ”¾é”
                redisTemplate.delete("idempotent:" + key);
            }
        } else {
            // 6. å…¶ä»–çº¿ç¨‹å¤„ç†ä¸­ï¼Œè¿”å›ç­‰å¾…ç»“æœ
            throw new ServiceException("è¯·æ±‚å¤„ç†ä¸­ï¼Œè¯·ç¨åå†è¯•");
        }
    }
}
```

**3. æ¶ˆæ¯å»é‡è®¾è®¡**

```java
// ç”Ÿäº§è€…ç«¯ç”Ÿæˆæ¶ˆæ¯ID
@Service
public class OrderProducer {
    @Autowired
    private KafkaTemplate<String, String> kafkaTemplate;
    
    public void sendOrder(OrderDTO order) {
        // 1. ç”Ÿæˆå”¯ä¸€æ¶ˆæ¯ID (UUIDæˆ–é›ªèŠ±ç®—æ³•)
        String messageId = UUID.randomUUID().toString();
        
        // 2. è®¾ç½®æ¶ˆæ¯å¤´
        ProducerRecord<String, String> record = new ProducerRecord<>("order_topic", JSON.toJSONString(order));
        record.headers().add(new RecordHeader("MESSAGE_ID", messageId.getBytes()));
        
        // 3. å‘é€æ¶ˆæ¯
        kafkaTemplate.send(record);
    }
}

// æ¶ˆè´¹è€…ç«¯è·å–æ¶ˆæ¯ID
@KafkaListener(topics = "order_topic")
public void consume(ConsumerRecord<String, String> record) {
    // è·å–æ¶ˆæ¯ID
    String messageId = "";
    for (Header header : record.headers()) {
        if ("MESSAGE_ID".equals(header.key())) {
            messageId = new String(header.value());
            break;
        }
    }
    
    // ä½¿ç”¨æ¶ˆæ¯IDè¿›è¡Œå¹‚ç­‰å¤„ç†
    orderService.processOrder(messageId, JSON.parseObject(record.value(), OrderDTO.class));
}
```

---